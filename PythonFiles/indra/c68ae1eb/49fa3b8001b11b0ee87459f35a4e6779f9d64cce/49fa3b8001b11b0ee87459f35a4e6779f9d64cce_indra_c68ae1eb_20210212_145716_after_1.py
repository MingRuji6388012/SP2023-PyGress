import logging
from collections import Counter
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from indra.statements import get_all_descendants, Statement


logger = logging.getLogger(__name__)


class SklearnBase(object):
    """Base class to wrap an Sklearn model with statement preprocessing.

    Parameters
    ----------
    model : sklearn or similar model
        Any instance of a classifier object supporting the methods `fit`
        and `predict_proba`.

    If using a dataframe, it should have columns corresponding to those
    generated by the IndraNets:

    agA_ns
    agA_id
    agA_name
    agB_ns
    agB_id
    agB_name
    stmt_type
    evidence_count
    stmt_hash
    residue
    position
    source_counts
    belief
    """
    def __init__(self, model):
        self.model = model

    def stmts_to_matrix(self, stmts, *args, **kwargs):
        raise NotImplementedError('Need to implement the stmts_to_matrix '
                                   'method')

    def df_to_matrix(self, df, *args, **kwargs):
        raise NotImplementedError('Need to implement the df_to_matrix '
                                   'method')

    def _to_matrix(self, stmt_data, *args, **kwargs):
        # Check if we have a dataframe or a list of statements
        # and call the appropriate *_to_matrix method
        if isinstance(stmt_data, pd.DataFrame):
            stmt_arr = self.df_to_matrix(stmt_data)
        # If not a DataFrame, assume have a list of stmts
        else:
            stmt_arr = self.stmts_to_matrix(stmt_data)
        return stmt_arr

    def fit(self, stmt_data, y_arr, *args, **kwargs):
        """Preprocess the stmt data and pass to sklearn model fit method."""
        # Check dimensions of stmts (x) and y_arr
        if len(stmt_data) != len(y_arr):
            raise ValueError("Number of stmts/rows must match length of y_arr.")
        # Get the data matrix based on the stmt list or stmt DataFrame
        stmt_arr = self._to_matrix(stmt_data)
        # Call the fit method of the internal sklearn model
        self.model.fit(stmt_arr, y_arr, *args, **kwargs)
        return self

    def predict_proba(self, stmt_data):
        # Call the prediction method of the internal sklearn model
        stmt_arr = self._to_matrix(stmt_data)
        return self.model.predict_proba(stmt_arr)

    def predict(self, stmt_data):
        stmt_arr = self.stmts_to_matrix(stmt_data)
        return self.model.predict(stmt_arr)

    def predict_log_proba(self, stmt_data):
        stmt_arr = self.stmts_to_matrix(stmt_data)
        return self.model.predict_log_proba(stmt_arr)


class CountsModel(SklearnBase):
    """Predictor based on source evidence counts and other stmt properties.

    Parameters
    ----------
    source_list : list of str
        List of strings denoting the evidence sources (evidence.source_api
        values) used for prediction.
    use_stmt_type : bool
        Whether to include statement type as a feature.
    use_num_members : bool
        Whether have a feature denoting the number of members of the statement.
        Primarily for stratifying belief predictions about Complex statements
        with more than two members.
    """
    def __init__(self, model, source_list, use_stmt_type=False,
                 use_num_members=False):
        # Call superclass constructor to store the model
        super(CountsModel, self).__init__(model)
        self.use_stmt_type = use_stmt_type
        self.use_num_members = use_num_members
        self.source_list = source_list

        # Build dictionary mapping INDRA Statement types to integers
        if use_stmt_type:
            all_stmt_types = get_all_descendants(Statement)
            self.stmt_type_map = {t.__name__: ix
                                  for ix, t in enumerate(all_stmt_types)}

    def stmts_to_matrix(self, stmts):
        # Add categorical features and collect source_apis
        cat_features = []
        stmt_sources = set()
        for stmt in stmts:
            # Collect all source_apis from stmt evidences
            for ev in stmt.evidence:
                stmt_sources.add(ev.source_api)
            # Collect non-source count features (e.g. type) from stmts
            feature_row = []
            if self.use_stmt_type:
                feature_row.append(self.stmt_type_map[type(stmt).__name__])
            # Only add a feature row if we're using some of the features.
            if feature_row:
                cat_features.append(feature_row)

        # Before proceeding, check whether all source_apis are in
        # source_list
        if stmt_sources.difference(set(self.source_list)):
            logger.warning("source_list does not include all source_apis "
                             "in the statement data.")

        # Get source count features
        num_cols = len(self.source_list)
        num_rows = len(stmts)
        x_arr = np.zeros((num_rows, num_cols))
        for stmt_ix, stmt in enumerate(stmts):
            sources = [ev.source_api for ev in stmt.evidence]
            src_ctr = Counter(sources)
            for src_ix, src in enumerate(self.source_list):
                x_arr[stmt_ix, src_ix] = src_ctr.get(src, 0)

        # If we have any categorical features, turn them into an array and
        # add them to matrix
        if cat_features:
            cat_arr = np.array(cat_features)
            x_arr = np.hstack((x_arr, cat_arr))
        return x_arr


    def df_to_matrix(self, df):
        required_cols = {'agA_id', 'agA_name', 'agA_ns', 'agB_id', 'agB_name',
                         'agB_ns', 'source_counts', 'stmt_hash', 'stmt_type'}
        # Make sure that the dataframe contains at least all of the above
        # columns
        if not required_cols.issubset(set(df.columns)):
            raise ValueError

        #import ipdb; ipdb.set_trace()
        # Add categorical features and collect source_apis
        cat_features = []
        stmt_sources = set()
        # For every statement entry in the dataframe...
        for rowtup in df.itertuples():
             # Collect non-source count features (e.g. type) from stmts
            feature_row = []
            if self.use_stmt_type:
                feature_row.append(self.stmt_type_map[rowtup.stmt_type])
            # Only add a feature row if we're using some of the features.
            if feature_row:
                cat_features.append(feature_row)

        # Before proceeding, check whether all source_apis are in
        # source_list
        if stmt_sources.difference(set(self.source_list)):
            logger.warning("source_list does not include all source_apis "
                             "in the statement data.")

        # Get source count features
        num_cols = len(self.source_list)
        num_rows = len(df)
        x_arr = np.zeros((num_rows, num_cols))
        for stmt_ix, rowtup in enumerate(df.itertuples()):
            for src_ix, src in enumerate(self.source_list):
                x_arr[stmt_ix, src_ix] = rowtup.source_counts.get(src, 0)

        # If we have any categorical features, turn them into an array and
        # add them to matrix
        if cat_features:
            cat_arr = np.array(cat_features)
            x_arr = np.hstack((x_arr, cat_arr))
        return x_arr


class LogLogisticRegression(LogisticRegression):
    def fit(self, x_train, y_train, *args, **kwargs):
        return super().fit(np.log(x_train + 1), y_train, *args, **kwargs)

    def predict(self, x_arr, *args, **kwargs):
        return super().predict(np.log(x_arr + 1), *args, **kwargs)

    def predict_proba(self, x_arr, *args, **kwargs):
        return super().predict_proba(np.log(x_arr + 1), *args, **kwargs)
